<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Work | Shuzhe Wang</title>
    <meta name="description" content="The description of the site.">
    <link rel="icon" href="/logo.png">
    
    <link rel="preload" href="/assets/css/0.styles.5bff1129.css" as="style"><link rel="preload" href="/assets/js/app.c52ee29c.js" as="script"><link rel="preload" href="/assets/js/8.38ded90f.js" as="script"><link rel="preload" href="/assets/js/4.90634f4b.js" as="script"><link rel="prefetch" href="/assets/js/10.ed4c6b3a.js"><link rel="prefetch" href="/assets/js/11.ed17755b.js"><link rel="prefetch" href="/assets/js/12.01827b90.js"><link rel="prefetch" href="/assets/js/13.5b6dbf0b.js"><link rel="prefetch" href="/assets/js/14.80689126.js"><link rel="prefetch" href="/assets/js/2.38c7697e.js"><link rel="prefetch" href="/assets/js/3.0ae01907.js"><link rel="prefetch" href="/assets/js/5.b070aab3.js"><link rel="prefetch" href="/assets/js/6.3d344552.js"><link rel="prefetch" href="/assets/js/7.b7fba462.js"><link rel="prefetch" href="/assets/js/9.636cb5f6.js">
    <link rel="stylesheet" href="/assets/css/0.styles.5bff1129.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar projects-page"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">Shuzhe Wang</span></a> <div class="links" style="max-width:nullpx;"><!----> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">Home</a></div><div class="nav-item"><a href="/about/" class="nav-link">About</a></div><div class="nav-item"><a href="/projects/" aria-current="page" class="nav-link router-link-exact-active router-link-active">Projects</a></div><div class="nav-item"><a href="https://github.com/ffrivera0" target="_blank" rel="noopener noreferrer" class="nav-link external">
  GitHub
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">Home</a></div><div class="nav-item"><a href="/about/" class="nav-link">About</a></div><div class="nav-item"><a href="/projects/" aria-current="page" class="nav-link router-link-exact-active router-link-active">Projects</a></div><div class="nav-item"><a href="https://github.com/ffrivera0" target="_blank" rel="noopener noreferrer" class="nav-link external">
  GitHub
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div> <!----></nav>  <!----> </div> <div class="page"> <div class="content"><h1 id="work">Work</h1> <p>Here are some works of mine ðŸ“š</p> <h2 id="publications">Publications</h2> <div class="md-card show-border"><!----> <div class="card-content"><p><strong>Visual Localization via Few-Shot Scene Region Classification (3DV 2022)</strong></p> <p>Siyan Dong*,  <strong>Shuzhe Wang*</strong>, Yixin Zhuang, Juho Kannala, Marc Pollefeys, Baoquan Chen</p> <p>Visual (re)localization addresses the problem of estimating the 6-DoF (Degree of Freedom) camera pose of a query image captured in a known scene, which is a key building block of many computer vision and robotics applications. Recent advances in structure-based localization solve this problem by memorizing the mapping from image pixels to scene coordinates with neural networks to build 2D-3D correspondences for camera pose optimization. However, such memorization requires training by amounts of posed images in each scene, which is heavy and inefficient. On the contrary, few-shot images are usually sufficient to cover the main regions of a scene for a human operator to perform visual localization. In this paper, we propose a scene region classification approach to achieve fast and effective scene memorization with few-shot images. Our insight is leveraging a) pre-learned feature extractor, b) scene region classifier, and c) meta-learning strategy to accelerate training while mitigating overfitting. We evaluate our method on both indoor and outdoor benchmarks. The experiments validate the effectiveness of our method in the few-shot setting, and the training time is significantly reduced to only a few minutes.</p> <p>[<a href="https://arxiv.org/pdf/2208.06933.pdf" target="_blank" rel="noopener noreferrer">PDF<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>] [<a href="https://github.com/siyandong/SRC" target="_blank" rel="noopener noreferrer">Code<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>] [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:oprcFyaIL5UJ:scholar.google.com/&amp;output=citation&amp;scisdr=CgXXTOCrEKzvtgUwYIc:AAGBfm0AAAAAYYA2eIcD37_zmmr5rIQ4xQV_20KiZSAc&amp;scisig=AAGBfm0AAAAAYYA2eHBNQKP9nK_VytByFwSiCySfOMT1&amp;scisf=4&amp;ct=citation&amp;cd=-1&amp;hl=zh-CN" target="_blank" rel="noopener noreferrer">BibTeX<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p></div></div> <div class="md-card show-border"><!----> <div class="card-content"><p><strong>Digging Into Self-Supervised Learning of Feature Descriptors (3DV 2021)</strong></p> <p>Iaroslav Melekhov, Zakaria Laskar, Xiaotian Li, <strong>Shuzhe Wang*</strong>, Juho Kannala</p> <p>Fully-supervised CNN-based approaches for learning local image descriptors have shown remarkable results in a wide range of geometric tasks. However, most of them require per-pixel ground-truth keypoint correspondence data which is difficult to acquire at scale. To address this challenge, recent weakly- and self-supervised methods can learn feature descriptors from relative camera poses or using only synthetic rigid transformations such as homographies. In this work, we focus on understanding the limitations of existing self-supervised approaches and propose a set of improvements that combined lead to powerful feature descriptors. We show that increasing the search space from in-pair to in-batch for hard negative mining brings consistent improvement. To enhance the discriminativeness of feature descriptors, we propose a coarse-to-fine method for mining local hard negatives from a wider search space by using global visual image descriptors. We demonstrate that a combination of synthetic homography transformation, color augmentation, and photorealistic image stylization produces useful representations that are viewpoint and illumination invariant. The feature descriptors learned by the proposed approach perform competitively and surpass their fully- and weakly-supervised counterparts on various geometric benchmarks such as image-based localization, sparse feature matching, and image retrieval.</p> <p>[<a href="https://arxiv.org/abs/2110.04773" target="_blank" rel="noopener noreferrer">PDF<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>] [<a href="">Code</a>] [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:oprcFyaIL5UJ:scholar.google.com/&amp;output=citation&amp;scisdr=CgXXTOELEKzvtX8fEwo:AAGBfm0AAAAAYvoZCwpx4EsEIrqVd_24fbqKPLgT_zk8&amp;scisig=AAGBfm0AAAAAYvoZC4xfMhYq_GDEdyUR6YYvxdBCQSr8&amp;scisf=4&amp;ct=citation&amp;cd=-1&amp;hl=zh-CN" target="_blank" rel="noopener noreferrer">BibTeX<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p></div></div> <div class="md-card show-border"><!----> <div class="card-content"><p><strong>Continual Learning for Image-Based Camera Localization (ICCV 2021)</strong></p> <p><strong>Shuzhe Wang*</strong> , Zakaria Laskar*, Iaroslav Melekhov, Xiaotian Li, Juho Kannala</p> <p>For several emerging technologies such as augmented reality, autonomous driving and robotics, visual localization is a critical component. Directly regressing camera pose/3D scene coordinates from the input image using deep neural networks has shown great potential. However, such methods assume a stationary data distribution with all scenes simultaneously available during training. In this paper, we approach the problem of visual localization in a continual learning setup -- whereby the model is trained on scenes in an incremental manner. Our results show that similar to the classification domain, non-stationary data induces catastrophic forgetting in deep networks for visual localization. To address this issue, a strong baseline based on storing and replaying images from a fixed buffer is proposed. Furthermore, we propose a new sampling method based on coverage score (Buff-CS) that adapts the existing sampling strategies in the buffering process to the problem of visual localization. Results demonstrate consistent improvements over standard buffering methods on two challenging datasets -- 7Scenes, 12Scenes, and also 19Scenes by combining the former scenes.</p> <p>[<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Continual_Learning_for_Image-Based_Camera_Localization_ICCV_2021_paper.pdf" target="_blank" rel="noopener noreferrer">PDF<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>] [<a href="https://github.com/AaltoVision/CL_HSCNet" target="_blank" rel="noopener noreferrer">Code<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>] [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:0eZeDLRtpiIJ:scholar.google.com/&amp;output=citation&amp;scisdr=CgXXTOCrEKzvtgU-X-4:AAGBfm0AAAAAYYA4R-6UQL1KrcShSX66Cm1XrAUZDaaN&amp;scisig=AAGBfm0AAAAAYYA4R5qS0WfBGqexQN5MhQCgGljBmWJr&amp;scisf=4&amp;ct=citation&amp;cd=-1&amp;hl=zh-CN" target="_blank" rel="noopener noreferrer">BibTeX<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p></div></div> <div class="md-card show-border"><!----> <div class="card-content"><p><strong>Hierarchical Scene Coordinate Classification and Regression for Visual Localization (CVPR2020)</strong></p> <p>Xiaotian Li, <strong>Shuzhe Wang</strong>, Yi Zhao, Jakob Verbeek, Juho Kannala</p> <p>Visual localization is critical to many applications in computer vision and robotics. To address single-image RGB localization, state-of-the-art feature-based methods match local descriptors between a query image and a pre-built 3D model. Recently, deep neural networks have been exploited to regress the mapping between raw pixels and 3D coordinates in the scene, and thus the matching is implicitly performed by the forward pass through the network. However, in a large and ambiguous environment, learning such a regression task directly can be difficult for a single network. In this work, we present a new hierarchical scene coordinate network to predict pixel scene coordinates in a coarse-to-fine manner from a single RGB image. The network consists of a series of output layers, each of them conditioned on the previous ones. The final output layer predicts the 3D coordinates and the others produce progressively finer discrete location labels. The proposed method outperforms the baseline regression-only network and allows us to train compact models which scale robustly to large environments. It sets a new state-of-the-art for single-image RGB localization performance on the 7-Scenes, 12-Scenes, Cambridge Landmarks datasets, and three combined scenes. Moreover, for large-scale outdoor localization on the Aachen Day-Night dataset, we present a hybrid approach which outperforms existing scene coordinate regression methods, and reduces significantly the performance gap w.r.t. explicit feature matching methods.</p> <p>[<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Hierarchical_Scene_Coordinate_Classification_and_Regression_for_Visual_Localization_CVPR_2020_paper.pdf" target="_blank" rel="noopener noreferrer">PDF<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>] [<a href="https://github.com/AaltoVision/hscnet" target="_blank" rel="noopener noreferrer">Code<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>] [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:EIIgXVmnYHEJ:scholar.google.com/&amp;output=citation&amp;scisdr=CgXT_HuZEIa99Kxm_N0:AAGBfm0AAAAAYSVg5N1OB_SSVoyNDEDW0B_6poErDXZA&amp;scisig=AAGBfm0AAAAAYSVg5CFuBHOzFJbO7OR9FJcqHkdNItRa&amp;scisf=4&amp;ct=citation&amp;cd=-1&amp;hl=en" target="_blank" rel="noopener noreferrer">BibTeX<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p></div></div> <h2 id="thesis">Thesis</h2> <div class="md-card show-border"><!----> <div class="card-content"><p><strong>Visual-Inertial Odometry Aided Temporal Camera Relocalization (Master's thesis)</strong></p> <p><strong>Shuzhe Wang</strong> , Supervisor: Prof. <a href="https://users.aalto.fi/~kannalj1/" target="_blank" rel="noopener noreferrer">Juho Kannala<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>; Advisor: <a href="https://scholar.google.com/citations?user=lht2z_IAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Li, Xiaotian<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>The goal of Temporal Camera Relocalization is to efficiently and effectively estimate the 6-DoF camera posew.r.tworld coordinate system. It is one of the fundamental problems in Augmented Reality and Autonomous Driving. However, most of the current approaches focus on one-shot image localization with an emphasis on a single RGB image for camera pose estimation, and the accuracy of TCR methods falls behind the SoTA one-shot methods even taking the time dependency into account.</p> <p>This thesis proposes a novel Temporal Camera Relocalization pipeline, which consists of three parts: global keyframe localization, local odometry, and fusion algorithms. The global localization has a hierarchical structure and can output image poses with high accuracy, the local tracking is provided by the latest visual-inertial odometry platform. Two fusion algorithms, global constraints and particle filter based method, are proposed in this thesis to utilize both global and local information for temporal camera relocalization. Experimental results show that both methods have promising performances with a mean error of less than0.48m/0.68â—¦in a space of30<em>20</em>2m3. The global constraints method achieves the best result with a mean errorof0.22m/0.2â—¦, the particle filter based method is robust to global pose estimation and has the ability to maintain the performance when the accuracy of global localization is significantly dropped.</p> <p>[<a href="">PDF</a>] [<a href="https://aaltodoc.aalto.fi/doc_public/export/bibtex/?url=https://aaltodoc.aalto.fi/handle/123456789/46042" target="_blank" rel="noopener noreferrer">BibTeX<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p></div></div> <h2 id="projects">Projects</h2> <div class="md-card show-border"><!----> <div class="card-content"><p><strong>Image-based large-scale indoor visual localization on mobile devices</strong></p> <p>The project creates a localization demo for large-scale indoor environment . I contributed to the building of database and SFM model, evaluating the SoAT approaches such as Hierarchical Localization, HSC-Net, DASC++, establishing a client-server connection.</p> <p>[<a href="https://drive.google.com/file/d/1CXAP-enEPagvxJCkJZnz0Ir-anjIE_0Y/view?usp=sharing" target="_blank" rel="noopener noreferrer">Video<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p></div></div> <div class="md-card show-border"><!----> <div class="card-content"><p><strong>Ship Thruster Interface</strong></p> <p>The project is to provide an intelligent maneuvering system for tugboats, enabling multiple vessels to coordinate their movements efficiently while assisting larger ships when entering and leaving the harbor. I mainly participated in the design of the control node and the communication between the ROS and the physical layer.</p> <p>[<a href="https://wiki.aalto.fi/download/attachments/151495776/Final%20report.pdf?api=v2" target="_blank" rel="noopener noreferrer">PDF<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>] [<a href="https://wiki.aalto.fi/display/AEEproject/Ship+thruster+interface" target="_blank" rel="noopener noreferrer">Page<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p></div></div></div> <div class="page-edit"><!----> <!----></div> <!----> </div> <!----></div></div>
    <script src="/assets/js/app.c52ee29c.js" defer></script><script src="/assets/js/8.38ded90f.js" defer></script><script src="/assets/js/4.90634f4b.js" defer></script>
  </body>
</html>
